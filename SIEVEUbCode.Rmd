---
title: "SIEVEUb"
author: "Jason McDermott"
date: "2/25/2019"
output: html_document
---

Rmarkdown to demonstrate use of SIEVE-Ub models for prediction of E3 ubiquitin ligase mimics from protein sequences. These blocks recreate much of the analysis from the PeerJ preprint at (https://peerj.com/preprints/27292/)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# as per reviewer #2 request - setting the random seed here such that code executes consistently below
# Note: we examine the effect of different random seeds on the stability of main result below, which
#       involves setting different random seeds
set.seed(1024)

# load R functions to run SVM cross-validation, etc.
source("SIEVEUb.R")
```

```{r data-ingest}

  # To generate features from a fasta file use the following command
  #    from the SIEVEServer code
  # KmerFeatures.py -f [input.faa] -o [outputbase] -m simple -k 14 -M reduced_alphabet_0
  
  
  # read in data file about examples (class and sequence family)
  ubex_classes = read.table("data/FamiliesConservative.txt", sep="\t", header=1, row.names=1)
  ubex_fact = factor(x=ubex_classes[,3], labels=c("positive", "negative"))
  names(ubex_fact) = rownames(ubex_classes)
  ubex_families = ubex_classes[,1]
  names(ubex_families) = rownames(ubex_classes)
  
    # this is a matrix of features from examples
  ubex_k14red0 = read.table("data/red0/ubligase_k14.red0.train", sep="\t", row.names=1, header=1, stringsAsFactors=F)

```

The following gets cross-validation performance results from multiple different kmer lengths and encodings. These sets of feature files and directories must be generated using the accompanying Python code in a somewhat manual fashion.
  
``` {r analysis}
  # writing a function to move through the sweep
  kmer_sweep = function(prefix, suffix, start=3, end=15, this_factor, this_families, niter=100) {
    results = c()
    for (i in start:end) {
      fname = sprintf("%s%d%s", prefix, i, suffix)
      data = read.table(fname, sep="\t", row.names=1, header=1, stringsAsFactors = F)
      
      svm_cv = family_bootstrap_svm(t(as.matrix(data[names(this_factor),])), this_factor, this_families, niter=niter)
      print(svm_cv$auc)
      results = c(results, svm_cv$auc)
    }
    return(results)
  }

  kmer_counts = function(prefix, suffix, start=3, end=15, this_factor, this_families, niter=100) {
    results = list()
    for (i in start:end) {
      fname = sprintf("%s%d%s", prefix, i, suffix)
      data = read.table(fname, sep="\t", row.names=1, header=1, stringsAsFactors = F)
      
      kmercounts = family_counts(data[names(this_factor),], this_families, this_factor)
      
      results = c(results, list(kmercounts))
    }
    return(results)
  }
  
  # this just performs the same analysis a bunch of times to see what the
  #   variability from train/test divisions is
  kmer_run = function(fname, this_factor, this_families, ntimes=10, niter=100, downsample_families=NA, data=NA) {
    if (is.na(data)) data = read.table(fname, sep="\t", row.names=1, header=1, stringsAsFactors = F)
    results = c()
    for (i in 1:ntimes) {
      svm_cv = family_bootstrap_svm(t(as.matrix(data[names(this_factor),])), this_factor, 
                                    this_families, niter=niter, downsample_families=downsample_families)
      #print(svm_cv$aucs)
      results = c(results, svm_cv$auc)
      #results = c(results, mean(svm_cv$aucs, na.rm=T))
    }
    return(results)
  }
  
  # this shows the cross-validation process on a series of kmer lengths using the simple hydrophobic/hydrophilic
  #      encoding. The feature vectors were pre-generated using the KmerFeatures.py code (see above)
  
  ubex_red0_sweep = kmer_sweep("data/red0/ubligase_k", ".red0.train", 3, 20, ubex_fact, ubex_families, 100)
  #ubex_red1_sweep = kmer_sweep("../analysis/red1/ubligase_k", ".red1.train", 3, 20, ubex_fact, ubex_families, 100)
  #ubex_red2_sweep = kmer_sweep("../analysis/red2/ubligase_k", ".red2.train", 3, 20, ubex_fact, ubex_families, 100)
  #ubex_red3_sweep = kmer_sweep("../analysis/red3/ubligase_k", ".red3.train", 3, 20, ubex_fact, ubex_families, 100)
  #ubex_red4_sweep = kmer_sweep("../analysis/red4/ubligase_k", ".red4.train", 3, 20, ubex_fact, ubex_families, 100)
  
  plot(y=ubex_red0_sweep, x=3:(length(ubex_red0_sweep)+2), ylab="AUC", xlab="kmer length", type="l", ylim=c(0.5,1))
  #plot(y=ubex_red1_sweep, x=3:(length(ubex_red1_sweep)+2), ylab="AUC", xlab="kmer length", type="l", ylim=c(0.5,1))
  
  #ubex_nat_ksweep = kmer_sweep("nat/ubligase_k", ".train", 3, 20, ubex_fact, ubex_families, 100)
  
  # to be clear about how this model was generated here is the code
  # For the final model we train on all the examples we have
  # Note that this is not the case for all the analysis we do of the model
  #    where we need to use cross-validation to determined performance in a
  #    robust manner
  sieveub_model = svm(x=ubex_k14red0[names(ubex_fact),], ubex_fact, probability=T)
  
  
```


``` {r kmer-counts}
# Using a simple approach to identify most discriminatory kmers
# this circumvents the cross-validation during recursive feature elimination
# and is solidly defensible in terms of pulling out the most predictive individual features.
ubex_red0_counts = kmer_counts("data/red0/ubligase_k", ".red0.train", 3, 20, ubex_fact, ubex_families, 100)
boxplot(sapply(1:18, function (i) list(ubex_red0_counts[[i]][,5])), names=3:20)
write.table(ubex_red0_counts[[12]], file="data/ubex_red0_counts.txt", sep="\t", quote=F)

ubex_red1_counts = kmer_counts("../analysis/red1/ubligase_k", ".red1.train", 3, 20, ubex_fact, ubex_families, 100)
boxplot(sapply(1:18, function (i) list(ubex_red1_counts[[i]][,5])), names=3:20)

ubex_red2_counts = kmer_counts("../analysis/red2/ubligase_k", ".red2.train", 3, 20, ubex_fact, ubex_families, 100)
boxplot(sapply(1:18, function (i) list(ubex_red2_counts[[i]][,5])), names=3:20)

ubex_red3_counts = kmer_counts("../analysis/red3/ubligase_k", ".red3.train", 3, 20, ubex_fact, ubex_families, 100)
boxplot(sapply(1:18, function (i) list(ubex_red3_counts[[i]][,5])), names=3:20)

ubex_red4_counts = kmer_counts("../analysis/red4/ubligase_k", ".red4.train", 3, 20, ubex_fact, ubex_families, 100)
boxplot(sapply(1:18, function (i) list(ubex_red4_counts[[i]][,5])), names=3:20)

ubex_nat_counts = kmer_counts("../analysis/nat/ubligase_k", ".train", 3, 20, ubex_fact, ubex_families, 100)
boxplot(sapply(1:18, function (i) list(ubex_nat_counts[[i]][,5])), names=3:20)


plot(3:20, sapply(1:18, function (i) max(ubex_red0_counts[[i]][,5])), ylim=c(0,0.6), col="red", type="l", ylab="Positive family bias", xlab="K")
lines(3:20, sapply(1:18, function (i) max(ubex_red1_counts[[i]][,5])), ylim=c(0,0.6), col="green")
lines(3:20, sapply(1:18, function (i) max(ubex_red2_counts[[i]][,5])), ylim=c(0,0.6), col="orange")
lines(3:20, sapply(1:18, function (i) max(ubex_red3_counts[[i]][,5])), ylim=c(0,0.6), col="purple")
lines(3:20, sapply(1:18, function (i) max(ubex_red4_counts[[i]][,5])), ylim=c(0,0.6), col="cyan")
lines(3:20, sapply(1:18, function (i) max(ubex_nat_counts[[i]][,5])), ylim=c(0,0.6), col="black")


# making the components of the new Figure 2 in the paper
# the final version of the figure was assembled in Illustrator
pdf("data/KmerCounts_red0_boxplot.pdf", width=10, height=4)
boxplot(sapply(1:18, function (i) list(ubex_red0_counts[[i]][,5])), names=3:20, xlab="K", ylab="Predictive Score", ylim=c(-0.4,0.6))
dev.off()
pdf("data/KmerCounts_red1_boxplot.pdf", width=10, height=4)
boxplot(sapply(1:18, function (i) list(ubex_red1_counts[[i]][,5])), names=3:20, xlab="K", ylab="Predictive Score", ylim=c(-0.4,0.6))
dev.off()
pdf("data/KmerCounts_red2_boxplot.pdf", width=10, height=4)
boxplot(sapply(1:18, function (i) list(ubex_red2_counts[[i]][,5])), names=3:20, xlab="K", ylab="Predictive Score", ylim=c(-0.4,0.6))
dev.off()
pdf("data/KmerCounts_red3_boxplot.pdf", width=10, height=4)
boxplot(sapply(1:18, function (i) list(ubex_red3_counts[[i]][,5])), names=3:20, xlab="K", ylab="Predictive Score", ylim=c(-0.4,0.6))
dev.off()
pdf("data/KmerCounts_red4_boxplot.pdf", width=10, height=4)
boxplot(sapply(1:18, function (i) list(ubex_red4_counts[[i]][,5])), names=3:20, xlab="K", ylab="Predictive Score", ylim=c(-0.4,0.6))
dev.off()
pdf("data/KmerCounts_nat_boxplot.pdf", width=10, height=4)
boxplot(sapply(1:18, function (i) list(ubex_nat_counts[[i]][,5])), names=3:20, xlab="K", ylab="Predictive Score", ylim=c(-0.4,0.6))
dev.off()

# scan sets of the top N best features for best predictive models
# this plot is not referenced in the paper, but shows that the top most predictive features
# can be combinined into predictive models - and that these are roughly equivalent over a range
# of top features
k14topmodels_results = sapply(2:50, function (i) list(kmer_run("dummy", ubex_fact, ubex_families, ntimes=1, data=ubex_k14red0[,names(sort(ubex_red0_counts[[12]][,5], decreasing=T))[1:i]])))

plot(2:50, k14topmodels, type="l", xlab="Top N features", ylab="AUC")

# choosing 10 as a reasonable number of features that gives a good predictive ability
# This is the minimal model
minimal10_features = names(sort(ubex_red0_counts[[12]][,5], decreasing=T))[1:10]
sieveub_minimal10_model = svm(x=ubex_k14red0[names(ubex_fact),minimal10_features], ubex_fact, probability=T)
write.table(minimal10_features, file="data/minimal_model_10features.txt", sep="\t", quote=F)

# the min10 model captures a bunch of the positive examples (maybe half) but fails to get the remainder
# we'll extend to min30 and check on that
minimal30_features = names(sort(ubex_red0_counts[[12]][,5], decreasing=T))[1:30]
sieveub_minimal30_model = svm(x=ubex_k14red0[names(ubex_fact),minimal30_features], ubex_fact, probability=T)
write.table(minimal30_features, file="data/minimal_model_30features.txt", sep="\t", quote=F)
save(sieveub_minimal30_model, file="data/SIEVEUb_min30_model.Rdata")

# the min30 model does better - but still not perfect (~80% of the positives)
# NOTE: the 'probability' output from the prediction is REVERSED of what it should be
# negative probability is actually positive probability
# extend to min50 and check
minimal50_features = names(sort(ubex_red0_counts[[12]][,5], decreasing=T))[1:50]
sieveub_minimal50_model = svm(x=ubex_k14red0[names(ubex_fact),minimal50_features], ubex_fact, probability=T)
write.table(minimal50_features, file="data/minimal_model_50features.txt", sep="\t", quote=F)
save(sieveub_minimal50_model, file="data/SIEVEUb_min50_model.Rdata")

# the min50 model does better - but still not perfect (~84% of the positives)
# extend to min100 and check
minimal100_features = names(sort(ubex_red0_counts[[12]][,5], decreasing=T))[1:100]
sieveub_minimal100_model = svm(x=ubex_k14red0[names(ubex_fact),minimal100_features], ubex_fact, probability=T)
write.table(minimal100_features, file="data/minimal_model_100features.txt", sep="\t", quote=F)
save(sieveub_minimal100_model, file="data/SIEVEUb_min100_model.Rdata")

# min100 is even better 88% of positives
# extend to 200 to check improvement
minimal200_features = names(sort(ubex_red0_counts[[12]][,5], decreasing=T))[1:200]
sieveub_minimal200_model = svm(x=ubex_k14red0[names(ubex_fact),minimal200_features], ubex_fact, probability=T)
write.table(minimal200_features, file="data/minimal_model_200features.txt", sep="\t", quote=F)
save(sieveub_minimal200_model, file="data/SIEVEUb_min200_model.Rdata")

# min200 is even better 90% of positives
# extend to 1000 to check improvement
minimal1000_features = names(sort(ubex_red0_counts[[12]][,5], decreasing=T))[1:1000]
sieveub_minimal1000_model = svm(x=ubex_k14red0[names(ubex_fact),minimal1000_features], ubex_fact, probability=T)
write.table(minimal1000_features, file="data/minimal_model_1000features.txt", sep="\t", quote=F)
save(sieveub_minimal1000_model, file="data/SIEVEUb_min1000_model.Rdata")

# min1000 is even better 93% of positives
# extend to 6000 as a sanity check 
minimal6000_features = names(sort(ubex_red0_counts[[12]][,5], decreasing=T))[1:6000]
sieveub_minimal6000_model = svm(x=ubex_k14red0[names(ubex_fact),minimal6000_features], ubex_fact, probability=T)
write.table(minimal6000_features, file="data/minimal_model_6000features.txt", sep="\t", quote=F)
save(sieveub_minimal6000_model, file="data/SIEVEUb_min6000_model.Rdata"
     
# min6000 gave 96% accurate classification of positives and 99% accurate classification of negatives
# this means that there's something wrong with the 'best_model' - from RFE below


```

Figure 1 in the paper compares the distribution of AUC values obtained with the true hydrophobic/phydrophilic alphabet and a series of randomly chosen binary alphabets. These were generated using the command: 
KmerFeatures.py -f [input.faa] -o [outputbase] -m simple -k 14 -M reduced_alphabet_0 -r

``` {r random-alphabets}

# get results from the real alphabet
realresults = kmer_run("data/red0/ubligase_k14.red0.train", ubex_fact, ntimes=10, ubex_families)

# get results from the randomized alphabets
rndresults = c()
for (i in 1:10) {
  fname = sprintf("data/rnd0/ubligase_k14rnd0_v%d.train", i)
  res = kmer_run(fname, ubex_fact, ntimes=5, ubex_families)
  rndresults = c(rndresults, res)
}

boxplot(realresults, rndresults, names=c("Real RED0 Alphabet", "Random RED0 Alphabets"), ylab="AUC")

```
Figure 1 (see manuscript): The plot above shows that the real hydrophobic/hydrophilic amino acid alphabet is significantly better than random alphabets (binary with the same balance).


There is one very large family that dominates the positive examples with over 100 members. This block tests whether the presence of the large number of sequences affects results by comparing it with a version that downsamples large families to 20 members.

``` {r downsampling}
res_nodown = kmer_run("data/red0/ubligase_k14.red0.train", ubex_fact, ntimes=5, ubex_families)

# the option downsample_families was added to accomplish this
res_down = kmer_run("data/red0/ubligase_k14.red0.train", ubex_fact, ubex_families, ntimes=5, downsample_families=20)

boxplot(res_nodown, res_down, ylim=c(0,1))

```

The answer is that the large family only influences the results modestly.

Examine the cross-validation results on the model using different random seeds. That is, does starting with different random seeds affect the outcomes of the cross-validation process? To test this we will just run the process a few times with different random seeds and then see if there are differences between the resulting performance.

``` {r random-seed}
set.seed(1024)
res1 = kmer_run("data/red0/ubligase_k14.red0.train", ubex_fact, ubex_families, ntimes=5)
set.seed(4506)
res2 = kmer_run("data/red0/ubligase_k14.red0.train", ubex_fact, ubex_families, ntimes=5)
set.seed(1209092)
res3 = kmer_run("data/red0/ubligase_k14.red0.train", ubex_fact, ubex_families, ntimes=5)

t.test(res1, res2)
t.test(res2, res3)

```
The random seed chosen has little/no effect on the performance of the method.

Next examine the feature elimination for the model.
``` {r model-rfe}
set.seed(1024)
# this recreates Figure 2 from the original version of the paper. This is a commonly accepted method for performing RFE
# on a model but does not take into account potential overfitting by the model (no cross-validation)
model.rfe = svmrfeFeatureRanking(ubex_k14red0[names(ubex_fact),], ubex_fact)

# this is a more appropriate way to do it - doing a cross validation at each step
model.cv.rfe = svmrfeCrossFeatureRanking(ubex_k14red0[names(ubex_fact), ], ubex_fact, ubex_families, stepamount = 0.25, niter=10, fbs_niter=100)

plot(c(6535, sapply(1:(length(model.cv.rfe)-3), function (i) model.cv.rfe[[i]]$nfeatures)), c(sapply(1:(length(model.cv.rfe)-3), function (i) model.cv.rfe[[i]]$auc), 0.7), type="l", ylim=c(0.4,1), xlab="Features", ylab="AUC")

# here are the features in a minimal 13 feature model that performs reasonably well (AUC 0.84)

# list performance and number of features
cbind(sapply(1:length(model.cv.rfe), function (i) model.cv.rfe[[i]]$auc), sapply(1:length(model.cv.rfe), function (i) model.cv.rfe[[i]]$nfeatures))

# the best model based on this approach is one with a large number of features, >1000. This shows that
#   the model is predictive overall, but that the cross-validating recursive feature elimination
#   process doesn't do a good job at focusing on a minimal set of features. The minimal model is
#   derived above from our scoring metric.

# this model has the best performance at 0.9044 - it has a large number of features, but that's OK
# 2020-11-12: this may be incorrect. It does not accurately classify the training examples. This is either
#  a bug - the labels are scrambled in some way - or a feature - the rfe process results in features that
#  may not performa as well on training data (unclear how this could be the case)
best_model_features = model.cv.rfe[[6]]$features[1:1172]
write.table(best_model_features, file="data/best_model_1280features.txt", sep="\t", quote=F)

sieveub_best_model = svm(x=ubex_k14red0[names(ubex_fact),best_model_features], ubex_fact, probability=T)
save(sieveub_best_model, file="data/SIEVEUb_best_model.Rdata")
```


Supplemental Figure 2. These plots show that elimination of features shows a peak performance of about 0.91 with 1280 features. 

``` {r PATRIC-application}
# download the PATRIC genomes listed in PATRIC.genomes
# concatenate fasta files
# run:
# python3 KmerFeatures.py -f PATRIC_humanpath_reference.faa -k 14 -M reduced_alphabet_0 -F data/minimal_model_18features.idx -m simple -o data/PATRIC_k14red0_minimal_model_18features.txt


# Because the size of the PATRIC features for ~400K proteins we do not include this datafile
#      but the results can be reproduced by downloading the genomes listed (PATRIC.genomes) and
#      applying the KmerFeatures.py to that file, then uncommenting the following code.

#load("data/SIEVEUb_best_model.Rdata")
#patric_best = read.table("../PATRIC_k14red0_best_model_features.txt.txt", sep="\t", row.names=1, header=1, stringsAsFactors =F)
#patric_best_predictions = validate_svm(train_model=sieveub_best_model, test_data = t(patric_best))

#write.table(patric_best_predictions$predictions, file="data/patric_ubligase_predictions_best_model.txt", sep="\t", quote=F)
```

``` {r model-validation}
legionella_minimal10_examples = read.table("data/legionella_examples_k14red0_minimal10.txt", sep="\t", row.names=1, header=1, stringsAsFactors = F)
legionella_minimal10_predictions = validate_svm(train_model=sieveub_minimal10_model, test_data=t(legionella_minimal10_examples))

```
Application of the model to a reently identified E3 ubiquitin ligase effector from Legionella, RavN. This effector has no detectable sequence similarity with others and was not included in our dataset. However, it was shown that it has a structure similar to U-box ligases, and that it functions as an E3 ligase. Our approach is able to correctly identify it as a ubiquitin ligase.

